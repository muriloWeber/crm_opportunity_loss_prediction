{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62572a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef3ae7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'c:\\Users\\murilo.weber\\projetos_portfolio\\tcrm_opportunity_loss_prediction' já está no sys.path.\n",
      "DateFeatureEngineer importado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Importa o custom transformer\n",
    "import sys\n",
    "# Adiciona a raiz do projeto ao PYTHONPATH\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    print(f\"Adicionado '{project_root}' ao sys.path para importações.\")\n",
    "else:\n",
    "    print(f\"'{project_root}' já está no sys.path.\")\n",
    "from src.utils.custom_transformers import DateFeatureEngineer\n",
    "print(\"DateFeatureEngineer importado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a262e6",
   "metadata": {},
   "source": [
    "# --- 0. Carregamento do DataFrame Consolidado ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbc040f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame carregado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Define o caminho para a pasta 'processed'\n",
    "processed_data_path = '../data/processed/'\n",
    "file_path = os.path.join(processed_data_path, 'df_eda_consolidated.csv')\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"DataFrame carregado com sucesso!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Erro: Arquivo '{file_path}' não encontrado.\")\n",
    "    print(\"Certifique-se de que o notebook '01_data_understanding.ipynb' foi executado e salvou 'df_eda_consolidated.csv'.\")\n",
    "    exit() # Interrompe a execução se o arquivo não for encontrado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d313fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Primeiras 5 linhas do DataFrame ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>opportunity_id</th>\n",
       "      <th>sales_agent</th>\n",
       "      <th>product</th>\n",
       "      <th>account</th>\n",
       "      <th>deal_stage</th>\n",
       "      <th>engage_date</th>\n",
       "      <th>close_date</th>\n",
       "      <th>close_value</th>\n",
       "      <th>target</th>\n",
       "      <th>sector</th>\n",
       "      <th>year_established</th>\n",
       "      <th>revenue</th>\n",
       "      <th>employees</th>\n",
       "      <th>office_location</th>\n",
       "      <th>subsidiary_of</th>\n",
       "      <th>series</th>\n",
       "      <th>sales_price</th>\n",
       "      <th>manager</th>\n",
       "      <th>regional_office</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1C1I7A6R</td>\n",
       "      <td>Moses Frase</td>\n",
       "      <td>GTX Plus Basic</td>\n",
       "      <td>Cancity</td>\n",
       "      <td>Engaging</td>\n",
       "      <td>2016-10-20</td>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>1054.0</td>\n",
       "      <td>0</td>\n",
       "      <td>retail</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>718.62</td>\n",
       "      <td>2448.0</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GTX</td>\n",
       "      <td>1096.0</td>\n",
       "      <td>Dustin Brinkmann</td>\n",
       "      <td>Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Z063OYW0</td>\n",
       "      <td>Darcel Schlecht</td>\n",
       "      <td>GTXPro</td>\n",
       "      <td>Isdom</td>\n",
       "      <td>Prospecting</td>\n",
       "      <td>2016-10-25</td>\n",
       "      <td>2017-03-11</td>\n",
       "      <td>4514.0</td>\n",
       "      <td>0</td>\n",
       "      <td>medical</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>3178.24</td>\n",
       "      <td>4540.0</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Melvin Marxen</td>\n",
       "      <td>Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EC4QE1BX</td>\n",
       "      <td>Darcel Schlecht</td>\n",
       "      <td>MG Special</td>\n",
       "      <td>Cancity</td>\n",
       "      <td>Engaging</td>\n",
       "      <td>2016-10-25</td>\n",
       "      <td>2017-03-07</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>retail</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>718.62</td>\n",
       "      <td>2448.0</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MG</td>\n",
       "      <td>55.0</td>\n",
       "      <td>Melvin Marxen</td>\n",
       "      <td>Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MV1LWRNH</td>\n",
       "      <td>Moses Frase</td>\n",
       "      <td>GTX Basic</td>\n",
       "      <td>Codehow</td>\n",
       "      <td>Engaging</td>\n",
       "      <td>2016-10-25</td>\n",
       "      <td>2017-03-09</td>\n",
       "      <td>588.0</td>\n",
       "      <td>0</td>\n",
       "      <td>software</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>2714.90</td>\n",
       "      <td>2641.0</td>\n",
       "      <td>United States</td>\n",
       "      <td>Acme Corporation</td>\n",
       "      <td>GTX</td>\n",
       "      <td>550.0</td>\n",
       "      <td>Dustin Brinkmann</td>\n",
       "      <td>Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PE84CX4O</td>\n",
       "      <td>Zane Levy</td>\n",
       "      <td>GTX Basic</td>\n",
       "      <td>Hatfan</td>\n",
       "      <td>Engaging</td>\n",
       "      <td>2016-10-25</td>\n",
       "      <td>2017-03-02</td>\n",
       "      <td>517.0</td>\n",
       "      <td>0</td>\n",
       "      <td>services</td>\n",
       "      <td>1982.0</td>\n",
       "      <td>792.46</td>\n",
       "      <td>1299.0</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GTX</td>\n",
       "      <td>550.0</td>\n",
       "      <td>Summer Sewald</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  opportunity_id      sales_agent         product  account   deal_stage  \\\n",
       "0       1C1I7A6R      Moses Frase  GTX Plus Basic  Cancity     Engaging   \n",
       "1       Z063OYW0  Darcel Schlecht          GTXPro    Isdom  Prospecting   \n",
       "2       EC4QE1BX  Darcel Schlecht      MG Special  Cancity     Engaging   \n",
       "3       MV1LWRNH      Moses Frase       GTX Basic  Codehow     Engaging   \n",
       "4       PE84CX4O        Zane Levy       GTX Basic   Hatfan     Engaging   \n",
       "\n",
       "  engage_date  close_date  close_value  target    sector  year_established  \\\n",
       "0  2016-10-20  2017-03-01       1054.0       0    retail            2001.0   \n",
       "1  2016-10-25  2017-03-11       4514.0       0   medical            2002.0   \n",
       "2  2016-10-25  2017-03-07         50.0       0    retail            2001.0   \n",
       "3  2016-10-25  2017-03-09        588.0       0  software            1998.0   \n",
       "4  2016-10-25  2017-03-02        517.0       0  services            1982.0   \n",
       "\n",
       "   revenue  employees office_location     subsidiary_of series  sales_price  \\\n",
       "0   718.62     2448.0   United States               NaN    GTX       1096.0   \n",
       "1  3178.24     4540.0   United States               NaN    NaN          NaN   \n",
       "2   718.62     2448.0   United States               NaN     MG         55.0   \n",
       "3  2714.90     2641.0   United States  Acme Corporation    GTX        550.0   \n",
       "4   792.46     1299.0   United States               NaN    GTX        550.0   \n",
       "\n",
       "            manager regional_office  \n",
       "0  Dustin Brinkmann         Central  \n",
       "1     Melvin Marxen         Central  \n",
       "2     Melvin Marxen         Central  \n",
       "3  Dustin Brinkmann         Central  \n",
       "4     Summer Sewald            West  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n--- Primeiras 5 linhas do DataFrame ---\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09af36d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Informações do DataFrame ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8800 entries, 0 to 8799\n",
      "Data columns (total 19 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   opportunity_id    8800 non-null   object \n",
      " 1   sales_agent       8800 non-null   object \n",
      " 2   product           8800 non-null   object \n",
      " 3   account           7375 non-null   object \n",
      " 4   deal_stage        8800 non-null   object \n",
      " 5   engage_date       8300 non-null   object \n",
      " 6   close_date        6711 non-null   object \n",
      " 7   close_value       7487 non-null   float64\n",
      " 8   target            8800 non-null   int64  \n",
      " 9   sector            7375 non-null   object \n",
      " 10  year_established  7375 non-null   float64\n",
      " 11  revenue           7375 non-null   float64\n",
      " 12  employees         7375 non-null   float64\n",
      " 13  office_location   7375 non-null   object \n",
      " 14  subsidiary_of     1292 non-null   object \n",
      " 15  series            7320 non-null   object \n",
      " 16  sales_price       7320 non-null   float64\n",
      " 17  manager           8800 non-null   object \n",
      " 18  regional_office   8800 non-null   object \n",
      "dtypes: float64(5), int64(1), object(13)\n",
      "memory usage: 1.3+ MB\n",
      "None\n",
      "\n",
      "--- Fim do carregamento ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Informações do DataFrame ---\")\n",
    "print(df.info())\n",
    "print(\"\\n--- Fim do carregamento ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8359a0d",
   "metadata": {},
   "source": [
    "# --- 1. Preparação para o Pipeline: Definição de Features ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da65ff21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Colunas removidas ANTES do pipeline para formar X_preprocessor_input: ['opportunity_id', 'account', 'deal_stage'] ---\n",
      "Número de colunas após remoção: 16\n"
     ]
    }
   ],
   "source": [
    "# A coluna 'opportunity_duration_days' será criada PELO Custom Transformer.\n",
    "\n",
    "# As colunas que não são 'opportunity_id', 'account', 'deal_stage' e 'target'\n",
    "# serão as entradas para o X_preprocessor_input.\n",
    "# As colunas 'engage_date' e 'close_date' serão processadas pelo DateFeatureEngineer.\n",
    "# 'deal_stage' será removido pois não é uma feature para o modelo.\n",
    "\n",
    "columns_to_drop_for_X_before_pipeline = ['opportunity_id', 'account', 'deal_stage'] # Apenas as colunas que não são features para o modelo\n",
    "existing_cols_to_drop = [col for col in columns_to_drop_for_X_before_pipeline if col in df.columns]\n",
    "df_for_pipeline_input = df.drop(columns=existing_cols_to_drop, errors='ignore')\n",
    "\n",
    "print(f\"\\n--- Colunas removidas ANTES do pipeline para formar X_preprocessor_input: {existing_cols_to_drop} ---\")\n",
    "print(f\"Número de colunas após remoção: {df_for_pipeline_input.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbbdfd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DataFrame final processado (com colunas de data para o pipeline) salvo em: '../data/processed/df_processed_for_modeling.csv' ---\n"
     ]
    }
   ],
   "source": [
    "# Salvar df_processed_for_modeling.csv (ainda é útil para análise fora do pipeline)\n",
    "# A única diferença é que as colunas de data ainda estarão presentes neste CSV,\n",
    "# mas o pipeline principal as removerá internamente.\n",
    "processed_data_output_path = '../data/processed/'\n",
    "if not os.path.exists(processed_data_output_path):\n",
    "    os.makedirs(processed_data_output_path)\n",
    "    print(f\"Diretório '{processed_data_output_path}' criado.\")\n",
    "\n",
    "df_final_filename = os.path.join(processed_data_output_path, 'df_processed_for_modeling.csv')\n",
    "# Salva o DF com as datas, pois o pipeline as receberá.\n",
    "df_for_pipeline_input.to_csv(df_final_filename, index=False)\n",
    "print(f\"\\n--- DataFrame final processado (com colunas de data para o pipeline) salvo em: '{df_final_filename}' ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cded9ac1",
   "metadata": {},
   "source": [
    "# --- 2. Construção do Pipeline de Pré-processamento ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07b025af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensões de X (input para o pre-processador): (8800, 15)\n",
      "Dimensões de y: (8800,)\n"
     ]
    }
   ],
   "source": [
    "# Definindo X e y\n",
    "X_preprocessor_input = df_for_pipeline_input.drop('target', axis=1)\n",
    "y_target = df_for_pipeline_input['target']\n",
    "\n",
    "print(f\"Dimensões de X (input para o pre-processador): {X_preprocessor_input.shape}\")\n",
    "print(f\"Dimensões de y: {y_target.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef5c8e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sales_agent', 'product', 'engage_date', 'close_date', 'close_value', 'sector', 'year_established', 'revenue', 'employees', 'office_location', 'subsidiary_of', 'series', 'sales_price', 'manager', 'regional_office']\n"
     ]
    }
   ],
   "source": [
    "print(X_preprocessor_input.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112f978d",
   "metadata": {},
   "source": [
    "## 2.1. Definição das colunas para cada tipo de transformação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4590819a",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\n",
    "    'close_value',\n",
    "    'year_established',\n",
    "    'revenue',\n",
    "    'employees',\n",
    "    'sales_price'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b5ff716",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\n",
    "    'sales_agent',\n",
    "    'product',\n",
    "    'sector',\n",
    "    'office_location',\n",
    "    'subsidiary_of',\n",
    "    'series',\n",
    "    'manager',\n",
    "    'regional_office'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "013152f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_features = ['engage_date', 'close_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e4fef2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Features numéricas para escalonamento: ['close_value', 'year_established', 'revenue', 'employees', 'sales_price'] ---\n",
      "--- Features categóricas para One-Hot Encoding: ['sales_agent', 'product', 'sector', 'office_location', 'subsidiary_of', 'series', 'manager', 'regional_office'] ---\n",
      "--- Features de data para engenharia: ['engage_date', 'close_date'] ---\n"
     ]
    }
   ],
   "source": [
    "# Verificação de que as features existem em X_preprocessor_input\n",
    "missing_numeric = [f for f in numeric_features if f not in X_preprocessor_input.columns]\n",
    "missing_categorical = [f for f in categorical_features if f not in X_preprocessor_input.columns]\n",
    "missing_date = [f for f in date_features if f not in X_preprocessor_input.columns]\n",
    "\n",
    "if missing_numeric or missing_categorical or missing_date:\n",
    "    raise ValueError(f\"Features missing from X_preprocessor_input: Numeric: {missing_numeric}, Categorical: {missing_categorical}, Date: {missing_date}\")\n",
    "\n",
    "print(f\"\\n--- Features numéricas para escalonamento: {numeric_features} ---\")\n",
    "print(f\"--- Features categóricas para One-Hot Encoding: {categorical_features} ---\")\n",
    "print(f\"--- Features de data para engenharia: {date_features} ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54b4015",
   "metadata": {},
   "source": [
    "## 2.2. Criação do Pré-processador (ColumnTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56075ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este ColumnTransformer atuará SOMENTE sobre as colunas que estarão presentes\n",
    "# DEPOIS que o DateFeatureEngineer já tiver feito seu trabalho.\n",
    "# Ou seja, 'opportunity_duration_days' estará presente, e 'engage_date'/'close_date' não.\n",
    "\n",
    "# ATENÇÃO: A lista 'numeric_features' PRECISA conter 'opportunity_duration_days'\n",
    "# APÓS o DateFeatureEngineer ter atuado.\n",
    "# No fit do preprocessor, o DateFeatureEngineer vai criar essa coluna.\n",
    "# Então, o ColumnTransformer espera a 'opportunity_duration_days' na lista de features numéricas.\n",
    "\n",
    "numeric_features_with_duration = [\n",
    "    'close_value',\n",
    "    'year_established',\n",
    "    'revenue',\n",
    "    'employees',\n",
    "    'sales_price',\n",
    "    'opportunity_duration_days' # Adiciona a feature que será criada\n",
    "]\n",
    "\n",
    "preprocessor_steps = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num_pipeline', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numeric_features_with_duration), # Aplica a cols numéricas (incluindo a futura duração)\n",
    "\n",
    "        ('cat_pipeline', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),\n",
    "            ('encoder', OneHotEncoder(handle_unknown='ignore', drop='first'))\n",
    "        ]), categorical_features) # Aplica a cols categóricas\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cabf8f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora, o pipeline completo será: DateFeatureEngineer -> preprocessor_steps (ColumnTransformer)\n",
    "# e este será o 'preprocessor' final que será salvo.\n",
    "# Note que `X_preprocessor_input` ainda conterá as colunas de data originais.\n",
    "# O pipeline final tratará isso.\n",
    "\n",
    "# Criando o Pipeline completo de pré-processamento\n",
    "# (Primeiro o DateFeatureEngineer, depois o ColumnTransformer)\n",
    "preprocessor = Pipeline(steps=[\n",
    "    ('date_feature_engineer', DateFeatureEngineer()),\n",
    "    ('column_transformer_steps', preprocessor_steps)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e77d4b",
   "metadata": {},
   "source": [
    "## 2.3. Treinar o pre-processador (ColumnTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f83e6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Ajustando o Pipeline de Pré-processamento (ColumnTransformer) ---\n",
      "--- Pipeline de Pré-processamento ajustado com sucesso! ---\n"
     ]
    }
   ],
   "source": [
    "# Ele precisa ser ajustado nos dados de treino para aprender as escalas e categorias.\n",
    "# Para isso, usaremos o X_preprocessor_input completo para ajustá-lo.\n",
    "print(\"\\n--- Ajustando o Pipeline de Pré-processamento (ColumnTransformer) ---\")\n",
    "preprocessor.fit(X_preprocessor_input)\n",
    "\n",
    "print(\"--- Pipeline de Pré-processamento ajustado com sucesso! ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4348af47",
   "metadata": {},
   "source": [
    "# --- 3. Salvamento do Pipeline de Pré-processamento ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d62692db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Pipeline de Pré-processamento salvo com sucesso em: '../models/preprocessor_pipeline.joblib' ---\n",
      "--- Fim do Processamento de Dados para Pré-processador ---\n"
     ]
    }
   ],
   "source": [
    "models_dir = '../models/'\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "    print(f\"Diretório '{models_dir}' criado com sucesso.\")\n",
    "\n",
    "# Salvar APENAS o pipeline de pré-processamento\n",
    "preprocessor_pipeline_filename = os.path.join(models_dir, 'preprocessor_pipeline.joblib')\n",
    "joblib.dump(preprocessor, preprocessor_pipeline_filename) # Salva o ColumnTransformer diretamente\n",
    "\n",
    "print(f\"\\n--- Pipeline de Pré-processamento salvo com sucesso em: '{preprocessor_pipeline_filename}' ---\")\n",
    "print(\"--- Fim do Processamento de Dados para Pré-processador ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
